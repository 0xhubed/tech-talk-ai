{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Neural Networks: From Linear Regression to Deep Learning\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/neural_network_intro.png\"  style=\"width:800px;height:200px;\" ></center>\n",
    "</figure>\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In the previous notebooks, we learned:\n",
    "- **Model Representation**: $f_{w,b}(x) = wx + b$ (linear regression)\n",
    "- **Cost Function**: How to measure prediction errors\n",
    "- **Gradient Descent**: How to optimize parameters\n",
    "\n",
    "But what if the relationship between input and output **isn't linear**?\n",
    "\n",
    "This notebook introduces **Neural Networks** - the foundation of modern AI systems like ChatGPT, Claude, and image recognition.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand why linear models fail on non-linear data\n",
    "2. Build a simple neural network from scratch\n",
    "3. Implement activation functions (sigmoid, ReLU)\n",
    "4. Perform forward propagation\n",
    "5. Understand backpropagation intuitively\n",
    "6. Train a multi-layer network on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-cell",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_c_internal_utils' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manimation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FuncAnimation\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:161\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\matplotlib\\cbook.py:32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ExceptionInfo\u001b[39;00m:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    A class to carry exception information around.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[33;03m    users and result in incorrect tracebacks.\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_c_internal_utils' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('./leonteq.mplstyle')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem-statement",
   "metadata": {},
   "source": [
    "## 2. The Limitation of Linear Models\n",
    "\n",
    "Let's create a dataset where the relationship between x and y is **non-linear**.\n",
    "\n",
    "### Example: Housing Prices with Non-Linear Features\n",
    "\n",
    "Imagine housing prices don't just increase linearly with size. Perhaps:\n",
    "- Small houses (< 100mÂ²) are cheap\n",
    "- Medium houses (100-200mÂ²) have a steep price increase\n",
    "- Large houses (> 200mÂ²) plateau in value\n",
    "\n",
    "This is a **non-linear** relationship!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonlinear-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data (sigmoid-like)\n",
    "def generate_nonlinear_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate housing data with non-linear relationship\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 4, n_samples)\n",
    "    # Non-linear function: sigmoid-like curve\n",
    "    y = 400 / (1 + np.exp(-2*(x - 2))) + 100\n",
    "    # Add some noise\n",
    "    y += np.random.normal(0, 20, n_samples)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_nonlinear_data(100)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='Data Points')\n",
    "plt.xlabel('Size (100 mÂ²)')\n",
    "plt.ylabel('Price (1000s CHF)')\n",
    "plt.title('Non-Linear Housing Price Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(x_train)} data points\")\n",
    "print(f\"Size range: {x_train.min():.2f} to {x_train.max():.2f} (100 mÂ²)\")\n",
    "print(f\"Price range: {y_train.min():.0f} to {y_train.max():.0f} (1000s CHF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-fail",
   "metadata": {},
   "source": [
    "### Try Linear Regression (It Will Fail!)\n",
    "\n",
    "Let's use our previous linear model $f_{w,b}(x) = wx + b$ and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression using normal equation\n",
    "def fit_linear_regression(x, y):\n",
    "    \"\"\"Fit linear model using least squares\"\"\"\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    w = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n",
    "    b = y_mean - w * x_mean\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "w_linear, b_linear = fit_linear_regression(x_train, y_train)\n",
    "y_pred_linear = w_linear * x_train + b_linear\n",
    "\n",
    "# Calculate error\n",
    "mse_linear = np.mean((y_train - y_pred_linear)**2)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='Actual Data')\n",
    "plt.plot(x_train, y_pred_linear, 'r-', linewidth=2, label=f'Linear Model (MSE={mse_linear:.0f})')\n",
    "plt.xlabel('Size (100 mÂ²)')\n",
    "plt.ylabel('Price (1000s CHF)')\n",
    "plt.title('Linear Regression on Non-Linear Data - Poor Fit!')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLinear Model: f(x) = {w_linear:.2f}x + {b_linear:.2f}\")\n",
    "print(f\"Mean Squared Error: {mse_linear:.2f}\")\n",
    "print(f\"\\nâŒ The linear model cannot capture the non-linear relationship!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-intro",
   "metadata": {},
   "source": [
    "## 3. Enter Neural Networks\n",
    "\n",
    "### The Key Idea: Add Non-Linearity!\n",
    "\n",
    "A neural network is just:\n",
    "1. **Linear transformations** (like we did before)\n",
    "2. **Non-linear activation functions** (the magic ingredient!)\n",
    "3. **Stacked in layers** (to learn complex patterns)\n",
    "\n",
    "### Architecture of a Simple Neural Network\n",
    "\n",
    "```\n",
    "Input â†’ [Hidden Layer] â†’ [Output Layer] â†’ Prediction\n",
    "  x   â†’  [w1, b1, Ïƒ]  â†’   [w2, b2, Ïƒ]  â†’     Å·\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **w, b**: Weights and biases (like before)\n",
    "- **Ïƒ**: Activation function (non-linear transformation)\n",
    "\n",
    "### Common Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid: squashes values to (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU: Rectified Linear Unit - max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Tanh: squashes values to (-1, 1)\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Plot activation functions\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0].set_title('Sigmoid: Ïƒ(z) = 1/(1+e^(-z))')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(z, relu(z), 'r-', linewidth=2)\n",
    "axes[1].set_title('ReLU: max(0, z)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "axes[2].plot(z, tanh(z), 'g-', linewidth=2)\n",
    "axes[2].set_title('Tanh: (e^z - e^(-z))/(e^z + e^(-z))')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('z')\n",
    "    ax.set_ylabel('activation(z)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ¨ These non-linear functions allow neural networks to learn complex patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-prop",
   "metadata": {},
   "source": [
    "## 4. Building a Neural Network from Scratch\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "We'll build a 2-layer neural network:\n",
    "- **Input layer**: 1 feature (size)\n",
    "- **Hidden layer**: 4 neurons with ReLU activation\n",
    "- **Output layer**: 1 neuron (price prediction)\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{[1]} &= W^{[1]}x + b^{[1]} \\\\\n",
    "a^{[1]} &= \\text{ReLU}(z^{[1]}) \\\\\n",
    "z^{[2]} &= W^{[2]}a^{[1]} + b^{[2]} \\\\\n",
    "\\hat{y} &= z^{[2]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network for regression\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=4, output_size=1):\n",
    "        \"\"\"Initialize network with random weights\"\"\"\n",
    "        # He initialization for better convergence\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2/input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2/hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Store for backpropagation\n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Reshape X if needed\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.cache['X'] = X.T\n",
    "        self.cache['z1'] = np.dot(self.W1, X.T) + self.b1\n",
    "        self.cache['a1'] = relu(self.cache['z1'])\n",
    "        \n",
    "        # Layer 2  \n",
    "        self.cache['z2'] = np.dot(self.W2, self.cache['a1']) + self.b2\n",
    "        \n",
    "        return self.cache['z2'].flatten()\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Mean Squared Error\"\"\"\n",
    "        m = len(y_true)\n",
    "        return np.sum((y_pred - y_true)**2) / (2*m)\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backpropagation - compute gradients\"\"\"\n",
    "        m = len(y)\n",
    "        \n",
    "        # Output layer gradients\n",
    "        y_pred = self.cache['z2'].flatten()\n",
    "        dz2 = (y_pred - y).reshape(-1, 1) / m\n",
    "        dW2 = np.dot(dz2, self.cache['a1'].T)\n",
    "        db2 = np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(self.W2.T, dz2)\n",
    "        dz1 = da1 * (self.cache['z1'] > 0)  # ReLU derivative\n",
    "        dW1 = np.dot(dz1, self.cache['X'].T)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        \"\"\"Gradient descent parameter update\"\"\"\n",
    "        self.W1 -= learning_rate * gradients['dW1']\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1, print_every=100):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        cost_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            cost_history.append(cost)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients, learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:4d}: Cost = {cost:.4f}\")\n",
    "        \n",
    "        return cost_history\n",
    "\n",
    "print(\"âœ… Neural Network class defined!\")\n",
    "print(\"   - Forward propagation: Input â†’ Hidden â†’ Output\")\n",
    "print(\"   - Backpropagation: Compute gradients\")\n",
    "print(\"   - Training: Optimize with gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-nn",
   "metadata": {},
   "source": [
    "## 5. Train the Neural Network\n",
    "\n",
    "Now let's train our neural network on the non-linear housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train neural network\n",
    "nn = SimpleNeuralNetwork(input_size=1, hidden_size=8, output_size=1)\n",
    "\n",
    "print(\"Training Neural Network...\\n\")\n",
    "cost_history = nn.train(x_train, y_train, epochs=2000, learning_rate=0.01, print_every=200)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn.forward(x_train)\n",
    "mse_nn = np.mean((y_train - y_pred_nn)**2)\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Final MSE: {mse_nn:.2f}\")\n",
    "print(f\"Improvement over linear: {((mse_linear - mse_nn) / mse_linear * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: Model comparison\n",
    "ax1.scatter(x_train, y_train, alpha=0.5, label='Actual Data', s=30)\n",
    "ax1.plot(x_train, y_pred_linear, 'r-', linewidth=2, label=f'Linear (MSE={mse_linear:.0f})')\n",
    "ax1.plot(x_train, y_pred_nn, 'g-', linewidth=2, label=f'Neural Net (MSE={mse_nn:.0f})')\n",
    "ax1.set_xlabel('Size (100 mÂ²)')\n",
    "ax1.set_ylabel('Price (1000s CHF)')\n",
    "ax1.set_title('Model Comparison: Linear vs Neural Network')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Training progress\n",
    "ax2.plot(cost_history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Cost (MSE)')\n",
    "ax2.set_title('Training Progress: Cost vs Epochs')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ The neural network captures the non-linear relationship!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeper-networks",
   "metadata": {},
   "source": [
    "## 7. Experiment: Deeper Networks\n",
    "\n",
    "What happens with more layers? More neurons?\n",
    "\n",
    "Try changing:\n",
    "- `hidden_size`: Number of neurons (4, 8, 16)\n",
    "- `learning_rate`: Speed of learning (0.001, 0.01, 0.1)\n",
    "- `epochs`: Training iterations (1000, 5000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different architectures\n",
    "hidden_sizes = [2, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    nn_exp = SimpleNeuralNetwork(input_size=1, hidden_size=hidden_size, output_size=1)\n",
    "    cost_hist = nn_exp.train(x_train, y_train, epochs=1000, learning_rate=0.01, print_every=10000)\n",
    "    y_pred = nn_exp.forward(x_train)\n",
    "    mse = np.mean((y_train - y_pred)**2)\n",
    "    results.append({'hidden_size': hidden_size, 'mse': mse, 'predictions': y_pred})\n",
    "    print(f\"Hidden size {hidden_size:2d}: MSE = {mse:.2f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x_train, y_train, alpha=0.3, s=20, label='Data')\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm']\n",
    "for i, result in enumerate(results):\n",
    "    plt.plot(x_train, result['predictions'], colors[i], \n",
    "             label=f\"{result['hidden_size']} neurons (MSE={result['mse']:.0f})\",\n",
    "             linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Size (100 mÂ²)')\n",
    "plt.ylabel('Price (1000s CHF)')\n",
    "plt.title('Neural Network Performance vs Architecture')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Linear models fail on non-linear data** \n",
    "   - $f(x) = wx + b$ can only fit straight lines\n",
    "\n",
    "2. **Activation functions add non-linearity**\n",
    "   - ReLU, sigmoid, tanh allow learning complex patterns\n",
    "\n",
    "3. **Neural networks = stacked transformations**\n",
    "   - Each layer: Linear transformation â†’ Activation\n",
    "   - Multiple layers â†’ arbitrarily complex functions\n",
    "\n",
    "4. **Forward propagation computes predictions**\n",
    "   - Input â†’ Hidden layers â†’ Output\n",
    "\n",
    "5. **Backpropagation computes gradients**\n",
    "   - Chain rule to find $\\frac{\\partial J}{\\partial W}$ for each layer\n",
    "   - Enables gradient descent optimization\n",
    "\n",
    "6. **More neurons/layers = more capacity**\n",
    "   - But watch out for overfitting!\n",
    "\n",
    "---\n",
    "\n",
    "### The Bridge to Modern AI\n",
    "\n",
    "This simple 2-layer network with 8 neurons is the **foundation** of:\n",
    "- **Deep Learning**: 100s of layers, millions of neurons\n",
    "- **Convolutional Networks**: For image recognition\n",
    "- **Recurrent Networks**: For sequences (time series, text)\n",
    "- **Transformers**: The architecture behind GPT, Claude, BERT\n",
    "\n",
    "**Next:** Notebook 5 will preview how transformers extend these ideas to build language models like ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## 9. Optional Exercises\n",
    "\n",
    "### Challenge 1: Classification Task\n",
    "Modify the network to classify housing prices as \"cheap\" (<300k) or \"expensive\" (>300k).\n",
    "Hint: Use sigmoid activation in output layer!\n",
    "\n",
    "### Challenge 2: Different Activation Functions\n",
    "Replace ReLU with sigmoid or tanh in hidden layer. How does performance change?\n",
    "\n",
    "### Challenge 3: Learning Rate Impact\n",
    "Try learning rates of 0.001, 0.01, 0.1, 1.0. What happens?\n",
    "\n",
    "### Challenge 4: Add a Third Layer\n",
    "Extend the network to have 2 hidden layers. Does it improve?\n",
    "\n",
    "---\n",
    "\n",
    "## References & Further Reading\n",
    "\n",
    "- **Neural Networks and Deep Learning** by Michael Nielsen\n",
    "- **Deep Learning** by Goodfellow, Bengio, Courville\n",
    "- **3Blue1Brown Neural Network Series** (YouTube)\n",
    "- **Stanford CS231n** Convolutional Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
