{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Transformers: The Architecture Behind Modern AI\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/transformer_architecture.png\"  style=\"width:800px;height:300px;\" ></center>\n",
    "</figure>\n",
    "\n",
    "## From Neural Networks to Language Models\n",
    "\n",
    "**This notebook is currently an OUTLINE** - to be completed by Daniel Huber\n",
    "\n",
    "### The Journey So Far:\n",
    "\n",
    "1. **Linear Regression**: $f(x) = wx + b$ - Simple but limited\n",
    "2. **Cost Functions**: Measuring prediction errors with MSE\n",
    "3. **Gradient Descent**: Optimizing parameters iteratively\n",
    "4. **Neural Networks**: Non-linear transformations with activation functions\n",
    "\n",
    "### The Next Leap: Transformers\n",
    "\n",
    "**Transformers** (2017) revolutionized AI by introducing a new architecture that powers:\n",
    "- **ChatGPT** (OpenAI)\n",
    "- **Claude** (Anthropic) \n",
    "- **BERT** (Google)\n",
    "- **GPT-4**, **Gemini**, and virtually all modern language models\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. Why traditional neural networks struggle with sequences\n",
    "2. The attention mechanism - \"looking at the right place\"\n",
    "3. Self-attention: how transformers understand context\n",
    "4. Positional encoding: teaching position to the model\n",
    "5. The complete transformer architecture\n",
    "6. How transformers scale to billions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-1",
   "metadata": {},
   "source": [
    "## SECTION 1: The Problem with Sequences\n",
    "\n",
    "### TODO: Add content\n",
    "\n",
    "Topics to cover:\n",
    "- Why position matters in language\n",
    "- Limitations of feed-forward networks for text\n",
    "- RNNs and their sequential bottleneck\n",
    "- The need for parallel processing\n",
    "\n",
    "### Example: Word Order Matters\n",
    "```\n",
    "\"The cat sat on the mat\" ‚â† \"The mat sat on the cat\"\n",
    "```\n",
    "\n",
    "Traditional neural networks (from Notebook 4) process each input independently - they can't understand the **relationship** between words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "plt.style.use('./leonteq.mplstyle')\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO: Add transformer imports when implementing\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-2",
   "metadata": {},
   "source": [
    "## SECTION 2: Attention Mechanism - The Key Innovation\n",
    "\n",
    "### TODO: Implement attention visualization\n",
    "\n",
    "**The Core Idea:**\n",
    "> \"When processing each word, look at ALL other words and decide which ones are important\"\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): \"What am I looking for?\"\n",
    "- **K** (Key): \"What do I contain?\"\n",
    "- **V** (Value): \"What information do I have?\"\n",
    "- **d_k**: Dimension of keys (scaling factor)\n",
    "\n",
    "### Example: Attention in \"The animal didn't cross the street because it was too tired\"\n",
    "\n",
    "When processing \"it\":\n",
    "- High attention to \"animal\" (92%)\n",
    "- Low attention to \"street\" (5%)\n",
    "- Low attention to \"cross\" (3%)\n",
    "\n",
    "The model learns that \"it\" refers to \"animal\", not \"street\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement simple attention mechanism\n",
    "\n",
    "def simple_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        Output: Weighted sum of values\n",
    "        Attention weights: Softmax of Q¬∑K^T\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# TODO: Add visualization of attention weights as heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-3",
   "metadata": {},
   "source": [
    "## SECTION 3: Self-Attention - Looking Within\n",
    "\n",
    "### TODO: Explain self-attention vs regular attention\n",
    "\n",
    "**Self-Attention**: Each word attends to every other word in the same sentence\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one attention mechanism, use **multiple** in parallel:\n",
    "- Head 1: Learns syntactic relationships (subject-verb)\n",
    "- Head 2: Learns semantic relationships (noun-adjective)\n",
    "- Head 3: Learns long-range dependencies\n",
    "- ...\n",
    "- Head 8: Learns positional patterns\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "where each $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multihead-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-head attention\n",
    "# TODO: Visualize what different heads learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-4",
   "metadata": {},
   "source": [
    "## SECTION 4: Positional Encoding\n",
    "\n",
    "### TODO: Explain why we need position information\n",
    "\n",
    "**Problem**: Attention has no inherent notion of order!\n",
    "- \"Dog bites man\" vs \"Man bites dog\" would look identical\n",
    "\n",
    "**Solution**: Add positional information to embeddings\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "PE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **pos**: Position in sequence\n",
    "- **i**: Dimension index\n",
    "- **d**: Total embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement positional encoding\n",
    "\n",
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings\n",
    "    \n",
    "    Args:\n",
    "        max_len: Maximum sequence length\n",
    "        d_model: Embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        PE: Positional encoding matrix (max_len, d_model)\n",
    "    \"\"\"\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    position = np.arange(0, max_len).reshape(-1, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# TODO: Visualize positional encodings as heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-5",
   "metadata": {},
   "source": [
    "## SECTION 5: The Complete Transformer Architecture\n",
    "\n",
    "### TODO: Build complete transformer block\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         TRANSFORMER BLOCK           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  1. Input Embeddings                ‚îÇ\n",
    "‚îÇ     + Positional Encoding           ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  2. Multi-Head Self-Attention       ‚îÇ\n",
    "‚îÇ     + Residual Connection           ‚îÇ\n",
    "‚îÇ     + Layer Normalization           ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  3. Feed-Forward Network            ‚îÇ\n",
    "‚îÇ     (2-layer MLP with ReLU)         ‚îÇ\n",
    "‚îÇ     + Residual Connection           ‚îÇ\n",
    "‚îÇ     + Layer Normalization           ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  [Repeat N times]                   ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îÇ  4. Output Layer                    ‚îÇ\n",
    "‚îÇ     (Linear + Softmax)              ‚îÇ\n",
    "‚îÇ                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Multi-Head Attention**: Process relationships between all tokens\n",
    "2. **Feed-Forward Network**: Transform representations\n",
    "3. **Layer Normalization**: Stabilize training\n",
    "4. **Residual Connections**: Enable deep networks (100+ layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement complete transformer block\n",
    "# TODO: Use PyTorch or implement from scratch\n",
    "\n",
    "class TransformerBlock:\n",
    "    \"\"\"Single transformer encoder block\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Embedding dimension\n",
    "            n_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        # TODO: Initialize layers\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-6",
   "metadata": {},
   "source": [
    "## SECTION 6: Training and Scaling\n",
    "\n",
    "### TODO: Discuss training transformers\n",
    "\n",
    "### From Small to Large\n",
    "\n",
    "| Model | Parameters | Layers | Hidden Size | Heads |\n",
    "|-------|-----------|---------|-------------|-------|\n",
    "| BERT-Base | 110M | 12 | 768 | 12 |\n",
    "| GPT-2 | 1.5B | 48 | 1600 | 25 |\n",
    "| GPT-3 | 175B | 96 | 12288 | 96 |\n",
    "| GPT-4 | ~1.7T (estimated) | ? | ? | ? |\n",
    "\n",
    "### Key Training Techniques:\n",
    "1. **Pre-training**: Learn from massive unlabeled text\n",
    "2. **Fine-tuning**: Adapt to specific tasks\n",
    "3. **RLHF** (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "### Computational Requirements:\n",
    "- **GPT-3 training**: ~$4.6 million in cloud compute\n",
    "- **Training time**: Months on thousands of GPUs\n",
    "- **Dataset**: 45TB of text (Common Crawl, books, Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder-7",
   "metadata": {},
   "source": [
    "## SECTION 7: Practical Example - Mini Language Model\n",
    "\n",
    "### TODO: Implement tiny GPT-style model\n",
    "\n",
    "Build a character-level language model that:\n",
    "1. Learns from Shakespeare text\n",
    "2. Generates new text in similar style\n",
    "3. Uses 2-layer transformer\n",
    "4. ~1M parameters (tiny!)\n",
    "\n",
    "This will demonstrate:\n",
    "- Text tokenization\n",
    "- Training loop\n",
    "- Text generation\n",
    "- Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini-gpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement mini GPT\n",
    "# TODO: Train on small dataset\n",
    "# TODO: Generate text\n",
    "# TODO: Visualize attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applications",
   "metadata": {},
   "source": [
    "## SECTION 8: Modern Applications\n",
    "\n",
    "### TODO: Show real-world transformer applications\n",
    "\n",
    "### Language Models\n",
    "- **GPT-4**: Text generation, reasoning, coding\n",
    "- **Claude**: Long-context understanding (100K+ tokens)\n",
    "- **BERT**: Search, question answering\n",
    "\n",
    "### Beyond Text\n",
    "- **Vision Transformers (ViT)**: Image classification\n",
    "- **DALL-E**: Text-to-image generation  \n",
    "- **AlphaFold**: Protein structure prediction\n",
    "- **Whisper**: Speech recognition\n",
    "\n",
    "### Multimodal Models\n",
    "- **GPT-4V**: Vision + language\n",
    "- **Gemini**: Text + image + video + audio\n",
    "\n",
    "### The Future\n",
    "- Longer contexts (1M+ tokens)\n",
    "- More efficient architectures\n",
    "- Reasoning and planning capabilities\n",
    "- Specialized domain models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Transformer Revolution\n",
    "\n",
    "1. **Attention is All You Need** (2017 paper title)\n",
    "   - Self-attention replaces recurrence\n",
    "   - Parallel processing enables scaling\n",
    "\n",
    "2. **Core Innovations**:\n",
    "   - **Self-attention**: Model relationships between all positions\n",
    "   - **Multi-head attention**: Learn different relationship types\n",
    "   - **Positional encoding**: Preserve sequence order\n",
    "   - **Residual connections**: Enable deep networks\n",
    "\n",
    "3. **Why Transformers Dominate**:\n",
    "   - Scale efficiently to billions of parameters\n",
    "   - Transfer learning across tasks\n",
    "   - Capture long-range dependencies\n",
    "   - Generalize beyond training data\n",
    "\n",
    "4. **From Foundations to GPT**:\n",
    "   - **Notebook 1-3**: Linear models, optimization basics\n",
    "   - **Notebook 4**: Neural networks with non-linearity\n",
    "   - **Notebook 5**: Transformers - attention + scale = modern AI\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Explore Further:\n",
    "- **Hugging Face**: Pre-trained transformer models\n",
    "- **Andrej Karpathy's nanoGPT**: Minimal GPT implementation\n",
    "- **The Illustrated Transformer**: Jay Alammar's visual guide\n",
    "- **Attention is All You Need**: Original 2017 paper\n",
    "\n",
    "### Build Something:\n",
    "- Fine-tune a model on your data\n",
    "- Build a chatbot with GPT\n",
    "- Create custom embeddings\n",
    "- Experiment with vision transformers\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've journeyed from linear regression to the architecture powering ChatGPT. You now understand the foundations of modern AI! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Papers\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)\n",
    "\n",
    "### Tutorials\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Hugging Face Course](https://huggingface.co/course)\n",
    "\n",
    "### Code Repositories\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT) - Minimal GPT implementation\n",
    "- [minGPT](https://github.com/karpathy/minGPT) - Educational GPT\n",
    "- [Transformers Library](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Videos\n",
    "- [Andrej Karpathy: Let's Build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "- [3Blue1Brown: Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
